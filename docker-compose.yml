services:
  ai_engine:
    build: ./ai_engine
    container_name: ai_engine
    ports:
      - "5000:5000"
    environment:
      # Pass environment variables from the host .env file to the container
      # This assumes a .env file exists in the ai_engine directory
      - OLLAMA_URL=http://ollama:11434
      - CHROMADB_URL=http://chromadb:8000
    env_file:
      - ./ai_engine/.env
    depends_on:
      - chromadb
      - ollama
    networks:
      - ai_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3

  chromadb:
    build: ./chromadb_custom
    container_name: chromadb
    ports:
      - "8001:8000"
    volumes:
      - chromadb_data:/chroma/data
    networks:
      - ai_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v2/heartbeat"]
      interval: 30s
      timeout: 15s
      retries: 5

  ollama:
    build: ./ollama_custom
    container_name: ollama
    ports:
      - "11434:11434"
    networks:
      - ai_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5

  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    ports:
      - "5678:5678"
    volumes:
      - n8n_data:/home/node/.n8n
      # This is the shared directory where new files will be placed.
      - file_intake:/intake
    environment:
      # It's recommended to set this to your local timezone, e.g., "America/New_York"
      - GENERIC_TIMEZONE=America/New_York
    networks:
      - ai_network

  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    ports:
      - "9443:9443"
      - "9000:9000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    restart: unless-stopped
    networks:
      - ai_network

  ollama-webui:
    image: ghcr.io/ollama-webui/ollama-webui:main
    container_name: ollama-webui
    ports:
      - "8080:8080"
    environment:
      - 'OLLAMA_API_BASE_URL=http://ollama:11434'
    volumes:
      - ollama_webui_data:/app/backend/data
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - ai_network

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    restart: unless-stopped
    networks:
      - ai_network

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped
    networks:
      - ai_network

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped
    networks:
      - ai_network





  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    command: --config.file=/etc/alertmanager/alertmanager.yml
    restart: unless-stopped
    networks:
      - ai_network

  file_sorter:
    build: ./file_sorter
    container_name: file_sorter
    ports:
      - "5001:5001"
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
      - LLAVA_MODEL_NAME=llava
      - OCR_MODEL_NAME=llava # Or a dedicated OCR model
      - TARGET_BASE_DIR=/organized_files
      - LOG_FILE=/var/log/file_sorter.log
    volumes:
      - file_intake:/intake:ro # Mount the intake directory as read-only
      - organized_files:/organized_files # Mount a volume for organized files
      - /var/log/file_sorter:/var/log/file_sorter # For persistent logs
    depends_on:
      - ollama
    restart: unless-stopped
    networks:
      - ai_network

#  sunshine:
#    image: lscr.io/linuxserver/sunshine:latest
#    container_name: sunshine
#    ports:
#      - "47989:47989/tcp" # Sunshine Web UI
#      - "47990:47990/tcp" # Sunshine Stream
#      - "47991:47991/tcp" # Sunshine Stream
#      - "47992:47992/tcp" # Sunshine Stream
#      - "443:47984/tcp" # HTTPS for pairing (optional, but good to have)
#      - "47998:47998/udp" # Sunshine Stream
#      - "47999:47999/udp" # Sunshine Stream
#      - "48000:48000/udp" # Sunshine Stream
#      - "48010:48010/udp" # Sunshine Stream
#    volumes:
#      - sunshine_config:/config
#      - /dev/dri:/dev/dri # For GPU access
#      - /run/udev:/run/udev:ro # For udev events (input devices)
#      - /path/to/your/games:/games # IMPORTANT: Change this to your actual games/ROMs directory
#    environment:
#      - PUID=1000 # IMPORTANT: Change to your user's UID
#      - PGID=1000 # IMPORTANT: Change to your user's GID
#      - TZ=America/New_York # IMPORTANT: Change to your timezone
#    privileged: true # Required for GPU access and input device handling
#    restart: unless-stopped
#    networks:
#      - ai_network

volumes:
  chromadb_data:
  ollama_models:
  n8n_data:
  file_intake:
  portainer_data:
  ollama_webui_data:
  prometheus_data:
  grafana_data:

  alertmanager_data:
  organized_files:
  #sunshine_config: # Add this new volume

networks:
  ai_network:
    driver: bridge